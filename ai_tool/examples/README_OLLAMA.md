# Ollama ä½¿ç”¨æµ‹è¯•æŒ‡å—

æœ¬ç›®å½•åŒ…å«äº†Ollama AIæœåŠ¡çš„å®Œæ•´æµ‹è¯•ç”¨ä¾‹å’Œä½¿ç”¨ç¤ºä¾‹ã€‚

## å‰ç½®æ¡ä»¶

### 1. å®‰è£…Ollama

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# ä¸‹è½½å¹¶å®‰è£… https://ollama.ai/download
```

### 2. å¯åŠ¨OllamaæœåŠ¡

```bash
ollama serve
```

### 3. ä¸‹è½½æ¨¡å‹

```bash
# ä¸‹è½½å¸¸ç”¨æ¨¡å‹
ollama pull llama2
ollama pull llama3
ollama pull mistral
ollama pull codellama
```

### 4. éªŒè¯å®‰è£…

```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
ollama list

# æµ‹è¯•æ¨¡å‹
ollama run llama2
```

## æµ‹è¯•æ–‡ä»¶è¯´æ˜

### 1. `ollama_quick_test.go` - å¿«é€Ÿæµ‹è¯•
æœ€ç®€å•çš„æµ‹è¯•è„šæœ¬ï¼ŒéªŒè¯OllamaåŸºæœ¬åŠŸèƒ½ã€‚

```bash
cd examples
go run ollama_quick_test.go
```

**åŠŸèƒ½ï¼š**
- âœ… è¿æ¥æµ‹è¯•
- âœ… åŸºæœ¬èŠå¤©æµ‹è¯•
- âœ… æµå¼èŠå¤©æµ‹è¯•
- âœ… æ¨¡å‹åˆ—è¡¨æµ‹è¯•

### 2. `ollama_example.go` - å®Œæ•´ç¤ºä¾‹
åŒ…å«æ‰€æœ‰OllamaåŠŸèƒ½çš„å®Œæ•´ç¤ºä¾‹ã€‚

```bash
cd examples
go run ollama_example.go
```

**åŠŸèƒ½ï¼š**
- ğŸ”„ åŸºæœ¬èŠå¤©
- ğŸŒŠ æµå¼èŠå¤©
- ğŸ’¬ äº¤äº’å¼èŠå¤©
- ğŸ¯ å¤šæ¨¡å‹æµ‹è¯•
- ğŸ”§ ç®¡ç†å™¨é›†æˆ

### 3. `ollama_test.go` - å•å…ƒæµ‹è¯•
å®Œæ•´çš„å•å…ƒæµ‹è¯•å¥—ä»¶ã€‚

```bash
cd ..
go test -v -run TestOllama
```

**æµ‹è¯•è¦†ç›–ï¼š**
- åŸºæœ¬èŠå¤©åŠŸèƒ½
- æµå¼èŠå¤©åŠŸèƒ½
- æ¨¡å‹åˆ—è¡¨è·å–
- è‡ªå®šä¹‰é…ç½®
- å®¢æˆ·ç«¯ç®¡ç†å™¨
- é™çº§ç­–ç•¥
- é”™è¯¯å¤„ç†
- å¹¶å‘è¯·æ±‚
- æ€§èƒ½åŸºå‡†æµ‹è¯•

## ç¯å¢ƒå˜é‡é…ç½®

```bash
# å¯é€‰ï¼šè‡ªå®šä¹‰Ollamaåœ°å€
export OLLAMA_BASE_URL="http://localhost:11434/v1"

# å¯é€‰ï¼šè‡ªå®šä¹‰è¶…æ—¶æ—¶é—´
export OLLAMA_TIMEOUT="60s"

# å¯é€‰ï¼šè‡ªå®šä¹‰é‡è¯•æ¬¡æ•°
export OLLAMA_MAX_RETRIES="3"
```

## ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨

```go
package main

import (
    "fmt"
    "github.com/karosown/katool-go/ai_tool"
    "github.com/karosown/katool-go/ai_tool/aiconfig"
)

func main() {
    // åˆ›å»ºOllamaå®¢æˆ·ç«¯
    client, err := ai_tool.NewAIClientFromEnv(aiconfig.ProviderOllama)
    if err != nil {
        panic(err)
    }
    
    // å‘é€èŠå¤©è¯·æ±‚
    response, err := client.Chat(&aiconfig.ChatRequest{
        Model: "llama2",
        Messages: []aiconfig.Message{
            {Role: "user", Content: "Hello!"},
        },
    })
    
    if err != nil {
        panic(err)
    }
    
    fmt.Println(response.Choices[0].Message.Content)
}
```

### æµå¼èŠå¤©

```go
// æµå¼èŠå¤©
stream, err := client.ChatStream(&aiconfig.ChatRequest{
    Model: "llama2",
    Messages: []aiconfig.Message{
        {Role: "user", Content: "Tell me a story"},
    },
})

if err != nil {
    panic(err)
}

for response := range stream {
    if len(response.Choices) > 0 && response.Choices[0].Delta.Content != "" {
        fmt.Print(response.Choices[0].Delta.Content)
    }
}
```

### å¤šæä¾›è€…é™çº§

```go
// åˆ›å»ºç®¡ç†å™¨
manager := ai_tool.NewAIClientManager()

// æ·»åŠ å¤šä¸ªå®¢æˆ·ç«¯
manager.AddClientFromEnv(aiconfig.ProviderOpenAI)
manager.AddClientFromEnv(aiconfig.ProviderDeepSeek)
manager.AddClientFromEnv(aiconfig.ProviderOllama)

// ä½¿ç”¨é™çº§ç­–ç•¥
response, err := manager.ChatWithFallback(
    []aiconfig.ProviderType{
        aiconfig.ProviderOpenAI,
        aiconfig.ProviderDeepSeek,
        aiconfig.ProviderOllama, // æœ¬åœ°å¤‡ç”¨
    },
    request,
)
```

## å¸¸è§é—®é¢˜

### Q: è¿æ¥å¤±è´¥æ€ä¹ˆåŠï¼Ÿ
A: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
1. OllamaæœåŠ¡æ˜¯å¦è¿è¡Œï¼š`ollama serve`
2. ç«¯å£æ˜¯å¦æ­£ç¡®ï¼šé»˜è®¤11434
3. é˜²ç«å¢™æ˜¯å¦é˜»æ­¢è¿æ¥
4. æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½ï¼š`ollama list`

### Q: æ¨¡å‹ä¸å­˜åœ¨æ€ä¹ˆåŠï¼Ÿ
A: ä¸‹è½½æ‰€éœ€æ¨¡å‹ï¼š
```bash
ollama pull llama2
ollama pull mistral
```

### Q: å“åº”å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ
A: å¯ä»¥è°ƒæ•´é…ç½®ï¼š
```go
config := &aiconfig.Config{
    BaseURL: "http://localhost:11434/v1",
    Timeout: 120 * time.Second, // å¢åŠ è¶…æ—¶æ—¶é—´
    MaxRetries: 3,
}
```

### Q: å¦‚ä½•é€‰æ‹©æ¨¡å‹ï¼Ÿ
A: æ ¹æ®éœ€æ±‚é€‰æ‹©ï¼š
- `llama2`: é€šç”¨å¯¹è¯ï¼Œå¹³è¡¡æ€§èƒ½
- `llama3`: æ›´å¥½çš„æ¨ç†èƒ½åŠ›
- `mistral`: æ›´å¿«çš„å“åº”
- `codellama`: ä»£ç ç”Ÿæˆ

## æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹é€‰æ‹©
- å°æ¨¡å‹ï¼šå“åº”å¿«ï¼Œèµ„æºå ç”¨å°‘
- å¤§æ¨¡å‹ï¼šè´¨é‡é«˜ï¼Œèµ„æºå ç”¨å¤š

### 2. å‚æ•°è°ƒä¼˜
```go
request := &aiconfig.ChatRequest{
    Model: "llama2",
    Temperature: 0.7,  // æ§åˆ¶éšæœºæ€§
    MaxTokens: 100,    // é™åˆ¶è¾“å‡ºé•¿åº¦
}
```

### 3. å¹¶å‘æ§åˆ¶
```go
// é™åˆ¶å¹¶å‘è¯·æ±‚æ•°é‡
semaphore := make(chan struct{}, 5) // æœ€å¤š5ä¸ªå¹¶å‘
```

## æ•…éšœæ’é™¤

### 1. æ£€æŸ¥æ—¥å¿—
```bash
# æŸ¥çœ‹Ollamaæ—¥å¿—
ollama logs

# æŸ¥çœ‹ç³»ç»Ÿæ—¥å¿—
journalctl -u ollama
```

### 2. é‡å¯æœåŠ¡
```bash
# åœæ­¢æœåŠ¡
pkill ollama

# é‡æ–°å¯åŠ¨
ollama serve
```

### 3. æ¸…ç†ç¼“å­˜
```bash
# æ¸…ç†æ¨¡å‹ç¼“å­˜
ollama rm llama2
ollama pull llama2
```

## æ›´å¤šèµ„æº

- [Ollamaå®˜æ–¹æ–‡æ¡£](https://ollama.ai/docs)
- [æ¨¡å‹åº“](https://ollama.ai/library)
- [APIæ–‡æ¡£](https://github.com/ollama/ollama/blob/main/docs/api.md)
