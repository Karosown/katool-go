package main

import (
	"fmt"
	"log"
	"time"

	"github.com/karosown/katool-go/ai_tool"
	"github.com/karosown/katool-go/ai_tool/aiconfig"
)

// å¿«é€Ÿæµ‹è¯•OllamaåŠŸèƒ½
func main() {
	fmt.Println("ğŸš€ Ollama Quick Test")
	fmt.Println("===================")

	// 1. æ£€æŸ¥Ollamaè¿æ¥
	fmt.Println("\n1. æ£€æŸ¥Ollamaè¿æ¥...")
	if !testOllamaConnection() {
		fmt.Println("âŒ Ollamaè¿æ¥å¤±è´¥")
		fmt.Println("è¯·ç¡®ä¿ï¼š")
		fmt.Println("- Ollamaå·²å®‰è£…: https://ollama.ai/")
		fmt.Println("- OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ: ollama serve")
		fmt.Println("- è‡³å°‘æœ‰ä¸€ä¸ªæ¨¡å‹: ollama pull llama2")
		return
	}
	fmt.Println("âœ… Ollamaè¿æ¥æˆåŠŸ")

	// 2. æµ‹è¯•åŸºæœ¬èŠå¤©
	fmt.Println("\n2. æµ‹è¯•åŸºæœ¬èŠå¤©...")
	testBasicChat()

	// 3. æµ‹è¯•æµå¼èŠå¤©
	fmt.Println("\n3. æµ‹è¯•æµå¼èŠå¤©...")
	testStreamChat()

	// 4. æµ‹è¯•æ¨¡å‹åˆ—è¡¨
	fmt.Println("\n4. æµ‹è¯•æ¨¡å‹åˆ—è¡¨...")
	testModelList()

	fmt.Println("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
}

// testOllamaConnection æµ‹è¯•Ollamaè¿æ¥
func testOllamaConnection() bool {
	config := &aiconfig.Config{
		BaseURL: "http://localhost:11434/v1",
		Timeout: 10 * time.Second,
	}

	client, err := ai_tool.NewAIClient(aiconfig.ProviderOllama, config)
	if err != nil {
		log.Printf("åˆ›å»ºå®¢æˆ·ç«¯å¤±è´¥: %v", err)
		return false
	}

	// å°è¯•ç®€å•è¯·æ±‚
	response, err := client.Chat(&aiconfig.ChatRequest{
		Model: "llama2",
		Messages: []aiconfig.Message{
			{Role: "user", Content: "Hi"},
		},
		MaxTokens: 5,
	})

	if err != nil {
		log.Printf("è¿æ¥æµ‹è¯•å¤±è´¥: %v", err)
		return false
	}

	return response != nil && len(response.Choices) > 0
}

// testBasicChat æµ‹è¯•åŸºæœ¬èŠå¤©
func testBasicChat() {
	client, err := ai_tool.NewAIClientFromEnv(aiconfig.ProviderOllama)
	if err != nil {
		log.Printf("åˆ›å»ºå®¢æˆ·ç«¯å¤±è´¥: %v", err)
		return
	}

	response, err := client.Chat(&aiconfig.ChatRequest{
		Model: "llama2",
		Messages: []aiconfig.Message{
			{Role: "user", Content: "What is 2+2? Answer in one word."},
		},
		Temperature: 0.3,
		MaxTokens:   10,
	})

	if err != nil {
		log.Printf("åŸºæœ¬èŠå¤©å¤±è´¥: %v", err)
		return
	}

	if len(response.Choices) > 0 {
		fmt.Printf("âœ… å›ç­”: %s\n", response.Choices[0].Message.Content)
	}
}

// testStreamChat æµ‹è¯•æµå¼èŠå¤©
func testStreamChat() {
	client, err := ai_tool.NewAIClientFromEnv(aiconfig.ProviderOllama)
	if err != nil {
		log.Printf("åˆ›å»ºå®¢æˆ·ç«¯å¤±è´¥: %v", err)
		return
	}

	stream, err := client.ChatStream(&aiconfig.ChatRequest{
		Model: "llama2",
		Messages: []aiconfig.Message{
			{Role: "user", Content: "Count from 1 to 5"},
		},
		Temperature: 0.5,
		MaxTokens:   50,
	})

	if err != nil {
		log.Printf("æµå¼èŠå¤©å¤±è´¥: %v", err)
		return
	}

	fmt.Print("âœ… æµå¼å›ç­”: ")
	chunkCount := 0
	for response := range stream {
		chunkCount++
		if len(response.Choices) > 0 && response.Choices[0].Delta.Content != "" {
			fmt.Print(response.Choices[0].Delta.Content)
		}
	}
	fmt.Printf("\n   æ”¶åˆ° %d ä¸ªæ•°æ®å—\n", chunkCount)
}

// testModelList æµ‹è¯•æ¨¡å‹åˆ—è¡¨
func testModelList() {
	client, err := ai_tool.NewAIClientFromEnv(aiconfig.ProviderOllama)
	if err != nil {
		log.Printf("åˆ›å»ºå®¢æˆ·ç«¯å¤±è´¥: %v", err)
		return
	}

	models := client.GetModels()
	fmt.Printf("âœ… å¯ç”¨æ¨¡å‹: %v\n", models)

	// æµ‹è¯•æ¯ä¸ªå¯ç”¨æ¨¡å‹
	for _, model := range models {
		fmt.Printf("   æµ‹è¯•æ¨¡å‹: %s... ", model)

		response, err := client.Chat(&aiconfig.ChatRequest{
			Model: model,
			Messages: []aiconfig.Message{
				{Role: "user", Content: "OK"},
			},
			MaxTokens: 3,
		})

		if err != nil {
			fmt.Printf("âŒ\n")
			continue
		}

		if len(response.Choices) > 0 {
			fmt.Printf("âœ…\n")
		} else {
			fmt.Printf("âš ï¸\n")
		}
	}
}
