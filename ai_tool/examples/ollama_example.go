package main

import (
	"bufio"
	"fmt"
	"log"
	"os"
	"strings"
	"time"

	"github.com/karosown/katool-go/ai_tool"
	"github.com/karosown/katool-go/ai_tool/aiconfig"
)

func main() {
	fmt.Println("=== Ollama Chat Example ===")
	fmt.Println("ä½¿ç”¨æœ¬åœ°OllamaæœåŠ¡è¿›è¡ŒAIèŠå¤©")

	// æ£€æŸ¥Ollamaæ˜¯å¦å¯ç”¨
	if !checkOllamaAvailable() {
		fmt.Println("âŒ Ollamaä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿ï¼š")
		fmt.Println("1. Ollamaå·²å®‰è£…å¹¶è¿è¡Œ")
		fmt.Println("2. è‡³å°‘æœ‰ä¸€ä¸ªæ¨¡å‹å¯ç”¨ï¼ˆå¦‚ï¼šollama pull llama2ï¼‰")
		fmt.Println("3. OllamaæœåŠ¡è¿è¡Œåœ¨ http://localhost:11434")
		return
	}

	fmt.Println("âœ… Ollamaå¯ç”¨ï¼Œå¼€å§‹èŠå¤©...")

	// ç¤ºä¾‹1: åŸºæœ¬èŠå¤©
	fmt.Println("\n1. åŸºæœ¬èŠå¤©ç¤ºä¾‹:")
	basicChatExample()

	// ç¤ºä¾‹2: æµå¼èŠå¤©
	fmt.Println("\n2. æµå¼èŠå¤©ç¤ºä¾‹:")
	streamChatExample()

	// ç¤ºä¾‹3: äº¤äº’å¼èŠå¤©
	fmt.Println("\n3. äº¤äº’å¼èŠå¤©:")
	interactiveChatExample()

	// ç¤ºä¾‹4: å¤šæ¨¡å‹æµ‹è¯•
	fmt.Println("\n4. å¤šæ¨¡å‹æµ‹è¯•:")
	multiModelExample()
}

// basicChatExample åŸºæœ¬èŠå¤©ç¤ºä¾‹
func basicChatExample() {
	// åˆ›å»ºOllamaå®¢æˆ·ç«¯
	client, err := ai_tool.NewAIClientFromEnv(aiconfig.ProviderOllama)
	if err != nil {
		log.Printf("åˆ›å»ºOllamaå®¢æˆ·ç«¯å¤±è´¥: %v", err)
		return
	}

	// å‘é€èŠå¤©è¯·æ±‚
	response, err := client.Chat(&aiconfig.ChatRequest{
		Model: "llama2", // ä½¿ç”¨llama2æ¨¡å‹
		Messages: []aiconfig.Message{
			{Role: "user", Content: "Hello! What is Go programming language?"},
		},
		Temperature: 0.7,
		MaxTokens:   100,
	})

	if err != nil {
		log.Printf("èŠå¤©è¯·æ±‚å¤±è´¥: %v", err)
		return
	}

	if len(response.Choices) > 0 {
		fmt.Printf("ğŸ¤– Ollama: %s\n", response.Choices[0].Message.Content)
	}
}

// streamChatExample æµå¼èŠå¤©ç¤ºä¾‹
func streamChatExample() {
	// åˆ›å»ºOllamaå®¢æˆ·ç«¯
	client, err := ai_tool.NewAIClientFromEnv(aiconfig.ProviderOllama)
	if err != nil {
		log.Printf("åˆ›å»ºOllamaå®¢æˆ·ç«¯å¤±è´¥: %v", err)
		return
	}

	// å‘é€æµå¼èŠå¤©è¯·æ±‚
	stream, err := client.ChatStream(&aiconfig.ChatRequest{
		Model: "llama2",
		Messages: []aiconfig.Message{
			{Role: "user", Content: "Tell me a short story about a robot"},
		},
		Temperature: 0.8,
		MaxTokens:   200,
	})

	if err != nil {
		log.Printf("æµå¼èŠå¤©è¯·æ±‚å¤±è´¥: %v", err)
		return
	}

	fmt.Print("ğŸ¤– Ollama (æµå¼): ")

	// å¤„ç†æµå¼å“åº”
	for response := range stream {
		if len(response.Choices) > 0 && response.Choices[0].Delta.Content != "" {
			fmt.Print(response.Choices[0].Delta.Content)
		}
	}
	fmt.Println()
}

// interactiveChatExample äº¤äº’å¼èŠå¤©ç¤ºä¾‹
func interactiveChatExample() {
	// åˆ›å»ºOllamaå®¢æˆ·ç«¯
	client, err := ai_tool.NewAIClientFromEnv(aiconfig.ProviderOllama)
	if err != nil {
		log.Printf("åˆ›å»ºOllamaå®¢æˆ·ç«¯å¤±è´¥: %v", err)
		return
	}

	// èŠå¤©å†å²
	var chatHistory []aiconfig.Message

	// æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
	chatHistory = append(chatHistory, aiconfig.Message{
		Role:    "system",
		Content: "You are a helpful AI assistant. Please provide clear and concise answers.",
	})

	reader := bufio.NewReader(os.Stdin)

	fmt.Println("å¼€å§‹äº¤äº’å¼èŠå¤©ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰:")

	for {
		fmt.Print("\nğŸ‘¤ ä½ : ")
		input, _ := reader.ReadString('\n')
		input = strings.TrimSpace(input)

		if input == "exit" || input == "quit" {
			fmt.Println("ğŸ‘‹ å†è§ï¼")
			break
		}

		if input == "" {
			continue
		}

		// æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
		chatHistory = append(chatHistory, aiconfig.Message{
			Role:    "user",
			Content: input,
		})

		// é™åˆ¶å†å²é•¿åº¦
		if len(chatHistory) > 10 {
			chatHistory = append(chatHistory[:1], chatHistory[len(chatHistory)-9:]...)
		}

		// å‘é€è¯·æ±‚
		response, err := client.Chat(&aiconfig.ChatRequest{
			Model:       "llama2",
			Messages:    chatHistory,
			Temperature: 0.7,
			MaxTokens:   150,
		})

		if err != nil {
			fmt.Printf("âŒ é”™è¯¯: %v\n", err)
			continue
		}

		if len(response.Choices) > 0 {
			assistantReply := response.Choices[0].Message.Content
			fmt.Printf("ğŸ¤– Ollama: %s\n", assistantReply)

			// æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
			chatHistory = append(chatHistory, aiconfig.Message{
				Role:    "assistant",
				Content: assistantReply,
			})
		}
	}
}

// multiModelExample å¤šæ¨¡å‹æµ‹è¯•ç¤ºä¾‹
func multiModelExample() {
	// åˆ›å»ºOllamaå®¢æˆ·ç«¯
	client, err := ai_tool.NewAIClientFromEnv(aiconfig.ProviderOllama)
	if err != nil {
		log.Printf("åˆ›å»ºOllamaå®¢æˆ·ç«¯å¤±è´¥: %v", err)
		return
	}

	// è·å–å¯ç”¨æ¨¡å‹
	models := client.GetModels()
	fmt.Printf("å¯ç”¨æ¨¡å‹: %v\n", models)

	// æµ‹è¯•ä¸åŒæ¨¡å‹
	testModels := []string{"llama2", "llama3", "mistral", "codellama"}

	for _, model := range testModels {
		// æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
		modelAvailable := false
		for _, availableModel := range models {
			if availableModel == model {
				modelAvailable = true
				break
			}
		}

		if !modelAvailable {
			fmt.Printf("âš ï¸  æ¨¡å‹ %s ä¸å¯ç”¨ï¼Œè·³è¿‡\n", model)
			continue
		}

		fmt.Printf("\næµ‹è¯•æ¨¡å‹: %s\n", model)

		response, err := client.Chat(&aiconfig.ChatRequest{
			Model: model,
			Messages: []aiconfig.Message{
				{Role: "user", Content: "What is 2+2?"},
			},
			Temperature: 0.5,
			MaxTokens:   30,
		})

		if err != nil {
			fmt.Printf("âŒ æ¨¡å‹ %s æµ‹è¯•å¤±è´¥: %v\n", model, err)
			continue
		}

		if len(response.Choices) > 0 {
			fmt.Printf("âœ… %s: %s\n", model, response.Choices[0].Message.Content)
		}
	}
}

// checkOllamaAvailable æ£€æŸ¥Ollamaæ˜¯å¦å¯ç”¨
func checkOllamaAvailable() bool {
	// åˆ›å»ºOllamaå®¢æˆ·ç«¯
	config := &aiconfig.Config{
		BaseURL: "http://localhost:11434/v1",
		Timeout: 5 * time.Second,
	}

	client, err := ai_tool.NewAIClient(aiconfig.ProviderOllama, config)
	if err != nil {
		return false
	}

	// å°è¯•ç®€å•è¯·æ±‚
	response, err := client.Chat(&aiconfig.ChatRequest{
		Model: "llama2",
		Messages: []aiconfig.Message{
			{Role: "user", Content: "test"},
		},
		MaxTokens: 5,
	})

	return err == nil && response != nil
}

// ollamaManagerExample Ollamaåœ¨ç®¡ç†å™¨ä¸­çš„ä½¿ç”¨ç¤ºä¾‹
func ollamaManagerExample() {
	fmt.Println("\n=== Ollama Manager Example ===")

	// åˆ›å»ºå®¢æˆ·ç«¯ç®¡ç†å™¨
	manager := ai_tool.NewAIClientManager()

	// æ·»åŠ å¤šä¸ªå®¢æˆ·ç«¯
	providers := []aiconfig.ProviderType{
		aiconfig.ProviderOpenAI,   // äº‘ç«¯æœåŠ¡
		aiconfig.ProviderDeepSeek, // äº‘ç«¯æœåŠ¡
		aiconfig.ProviderOllama,   // æœ¬åœ°æœåŠ¡
	}

	// æ·»åŠ å®¢æˆ·ç«¯
	for _, provider := range providers {
		if err := manager.AddClientFromEnv(provider); err != nil {
			fmt.Printf("âš ï¸  æ— æ³•æ·»åŠ  %s å®¢æˆ·ç«¯: %v\n", provider, err)
		} else {
			fmt.Printf("âœ… æˆåŠŸæ·»åŠ  %s å®¢æˆ·ç«¯\n", provider)
		}
	}

	// åˆ—å‡ºå¯ç”¨å®¢æˆ·ç«¯
	availableClients := manager.ListClients()
	fmt.Printf("å¯ç”¨å®¢æˆ·ç«¯: %v\n", availableClients)

	// ä½¿ç”¨é™çº§ç­–ç•¥
	request := &aiconfig.ChatRequest{
		Model: "llama2", // ä½¿ç”¨Ollamaæ”¯æŒçš„æ¨¡å‹
		Messages: []aiconfig.Message{
			{Role: "user", Content: "Hello from fallback test!"},
		},
		Temperature: 0.7,
		MaxTokens:   50,
	}

	response, err := manager.ChatWithFallback(providers, request)
	if err != nil {
		fmt.Printf("âŒ é™çº§èŠå¤©å¤±è´¥: %v\n", err)
		return
	}

	if len(response.Choices) > 0 {
		fmt.Printf("âœ… é™çº§å“åº”: %s\n", response.Choices[0].Message.Content)
	}
}
